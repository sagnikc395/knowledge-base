import{f as s,a}from"./CCM6-gPc.js";import"./ChUk9lal.js";import{l as r,s as h}from"./D9W1QFau.js";import{M as u}from"./i8E_CZTo.js";const i={title:"Neural Networks from Scratch",tags:["100xdevs","ai","nlp","neural-networks"],date:"1/24/26"},{title:w,tags:y,date:b}=i;var c=s("<ul><li><strong>Neural Networks Architecture</strong> <ul><li>how machines actually learn</li> <li>![[Screenshot 2026-01-24 at 10.02.45 AM.png]]</li> <li>first one -> input layer</li> <li>last layer -> output layer</li> <li>layers in the middle -> hidden layer , generally n number of hidden layers</li> <li>the circular things are the neurons</li></ul></li> <li><strong>ML</strong> <ul><li>Goal: understanding the fundamental mechanism of pattern discovery of data. i.e mathematical relationships within large datasets.</li> <li>Training is simple, iterative loop of guessing and correcting: <ul><li>start with a guess -> random values</li> <li>measure error -> how far is the guess from the truth ?</li> <li>adjust -> modify the guess to be slightly less wrong</li> <li>repeat -> do this millions of time, until error zeros or very very close to zero.</li></ul></li> <li>![[Screenshot 2026-01-24 at 10.21.57 AM.png]]</li></ul></li> <li><strong>Neuron -> A Tiny Decision Maker</strong> <ul><li>Building block of AI is a simple mathematical function, not a biological mystery.</li> <li>It takes some numbers in, performs basic arithmetic ,and outputs a signal. <ul><li>Inputs -> numerical data points representing features (eg: house size, pixel intensity)</li> <li>weights and summation -> importance assigned to each input, combined into a single weighted sum.</li> <li>activation -> a non-linear function that decides whether the signal should be “fired” or not “fired”.</li> <li>![[Screenshot 2026-01-24 at 10.24.49 AM.png]]</li> <li>Weights -> <code>W1,W2,W3....</code> , something that the change the weight accordingly to change the loss</li> <li>Inputs -> Features</li> <li>non-linearity -> activation function</li></ul></li> <li>Anatomy of a neuron : <ul><li><code>output = activation(\\sigma wixi + b)</code></li> <li>weights -> w -> determine the strength and importance of each input signal</li> <li>bias (b) -> an offset that allows the neuron to shift its decision boundary</li> <li>activation -> a nonlinear function that decides if the signal should “fire”.</li></ul></li> <li>the need for activation functions : <ul><li>Consider <ul><li>Layer 1 -> <code>y := W1x + b1</code></li> <li>Layer 2 -> <code>z = W2y + b2</code></li></ul></li> <li>Combining them we get <ul><li><code>Z = W2(W1x+b1) + b2</code></li> <li><code>Z = (W2W1)X + (W2b1+b2)</code></li></ul></li> <li>this will then simplify to: <ul><li><code>z = W'x + b'</code></li> <li>where <ul><li><code>W' = W2W1</code></li> <li><code>b' = W2b1+b2</code></li></ul></li></ul></li> <li>so basically if we are stacking layers cause it will just results in another linear function.</li> <li>so 100 layers of this thing will be equivalent to just one.</li> <li>so if we add non-linearity to this , we can solve this function.</li></ul></li> <li>How Activation Function solves this : <ul><li>activation functions add “bends” to the math, preventing layers from collapsing into each other -> <strong>breaking the linearity</strong> and linear combination of operations.</li> <li><strong>Enabling Depth</strong> -> “secret sauce” that allows deep networks to learn complex, multi-layered representations.</li> <li><strong>Universal Approximation</strong> -> Non-linearity allows the networks to model any continuous functions, no matter how complex the function is.</li></ul></li> <li>Different Activations Functions <ul><li>sigmoid <ul><li>classic S-curve 0-1.</li> <li>default used for probabilistic ML.</li></ul></li> <li>tanh <ul><li>zero centered (-1 to 1)</li> <li>often provides faster convergence than sigmoid.</li></ul></li> <li>ReLU <ul><li>![[Screenshot 2026-01-24 at 10.55.52 AM.png]]</li> <li>max(0,x) -> simple ,efficient and enables deep networks.</li></ul></li> <li>Modern Variants <ul><li>GELU -> modern smooth version of ReLU.</li> <li>Swish</li></ul></li></ul></li></ul></li> <li><strong>Loss Functions</strong>: <ul><li>The thing where the feedback loop starts.</li> <li>I.e minimizing this score the ML systems get better</li> <li>loss = a single number measuring how wrong we are</li> <li>lower loss implies better predictions</li> <li>training goal of a ml system is to minimize the loss.</li> <li>most of the times we used mean squared error (MSE) to compute loss <ul><li><code>L = (1/n) \\sigma (predicition -target)^2</code></li> <li>Squares will make all the errors positive</li> <li>big errors will be penalized more than small errors</li> <li>good for regression tasks.</li></ul></li></ul></li> <li>Backpropogation: <ul><li>![[Screenshot 2026-01-24 at 11.06.04 AM.png]]</li> <li>how does the machine knows which weights to adjust to minimize loss ?</li> <li>it checks which nodes weights that we need to change.</li> <li>Intuition for backprop: <ul><li>in a big org, something went wrong, and why are things are down ? <ul><li>ceo would ask vp , vp to managers and the managers to employees</li> <li>they are backpropogating to find the error and to trying to optimize the process</li></ul></li> <li>basically like a chain of responsibility.</li> <li>error signal travel backwards from the output later through the hidden layers to the input.</li> <li>adjustment: each weight is adjusted proportionally to its contribution to the final mistake.</li></ul></li></ul></li> <li><strong>Learning Rate</strong>: <ul><li>a hyperparameter -> something that is tunable and can be changed.</li> <li>how fast or how slow we can tune the loss minimization</li> <li>![[Screenshot 2026-01-24 at 11.15.52 AM.png]]</li> <li>(loss with a 2d graph and 3d repr)</li> <li>loss landscape -> visualize how the loss looks for our loss function</li> <li>we try to move in the direction of the greatest gradient</li> <li>learning rate telling us how fast to go there <ul><li>high learning rate might take us directly or it might overshoot</li> <li>slow learning rate might take a lot of time to get to the optimum</li></ul></li></ul></li> <li><strong>Gradient Descent</strong>: <ul><li>Strategy: <ul><li>Step Downhill : Feel the slope and take a step in that direction that reduces the loss.</li> <li>Iterative Progress : repeat the process, taking step after step until we reach the bottom</li> <li>Goal : Reach the global minimum - the point where the model’s error is the minimum.</li></ul></li></ul></li> <li>Whole Training Loop: <ul><li>![[Screenshot 2026-01-24 at 11.31.22 AM.png]]</li></ul></li></ul>");function v(e,l){const t=r(l,["children","$$slots","$$events","$$legacy"]);u(e,h(()=>t,()=>i,{children:(o,d)=>{var n=c();a(o,n)},$$slots:{default:!0}}))}export{v as default,i as metadata};
