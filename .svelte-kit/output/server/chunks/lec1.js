import { G as sanitize_props, J as spread_props } from "./index.js";
import { M as MarkdownLayout } from "./MarkdownLayout.js";
const metadata = {
  "title": "Docker TerraForm and SQL",
  "tags": ["data-engineering", "terraform", "sql"],
  "date": "1/18/26"
};
const { title, tags, date } = metadata;
function Lec1_md($$renderer, $$props) {
  const $$sanitized_props = sanitize_props($$props);
  MarkdownLayout($$renderer, spread_props([
    $$sanitized_props,
    metadata,
    {
      children: ($$renderer2) => {
        $$renderer2.push(`<h3>Difference b/w an Data Engineer and and AI Engineer</h3> <ul><li>Data Engineer -> making sure all the data is captured, ready for analysis , clean and can be used.</li> <li>AI Engineer -> work on the AI aspect of the product , do prompt tuning, validate your model, verifiable rewards etc.</li></ul> <h3>Docker For Data Engineering:</h3> <ul><li>Docker is a way to separate what we have in our host machine and the application (that requires other software dependencies).</li> <li>What we have in Docker container is completely isolated from host machine.</li> <li>What we run has no effect from the host machine and has no effect from the host machine.</li> <li>Simplest Docker Command to check if Docker installed correctly: <ul><li><code>docker run hello-world</code></li> <li>Can run with interactive terminal as <code>docker run -it ubuntu</code> <ul><li>Whatever we do here is isolated from the host machine</li></ul></li> <li>To come back we do <code>docker run -it ubuntu</code></li> <li>But when we do this , the problem is the command is then not found.</li></ul></li> <li>Everytime we create an docker container from an Docker image. <ul><li>An instance of the Docker image is created -> it contains a complete snapshot of the operating system.</li> <li><img src="/images/migrated/data-engineering-zoomcamp/imgs/Screenshot%202026-01-18%20at%2011.48.05%20AM.png" alt="Screenshot 2026-01-18 at 11.48.05 AM.png"/></li> <li>It is stateless and doesn’t preserve the state.</li> <li>Instead of Ubuntu we can also use the direct python image of 3.13 directly also.</li></ul></li> <li>When installing the Python 3.13 image : <ul><li>python -> image name</li> <li>3.13.11-slim -> is the tag name</li></ul></li> <li>If we wanted the entry point to be a bash session and not python repl directly: <ul><li><code>docker run -it --entrypoint=bash python:3.13.11-slim</code></li></ul></li> <li>We can see stopped containers (which are saved somewhere as) <ul><li><code>docker ps -a</code></li> <li>even if we do <code>rm -rf /</code></li> <li>lists all the docker images that we executed and they have all the state</li> <li>can remove all the files as <code>docker ps -aq</code> -> gets the id <ul><li>then run as \`<code>docker rm</code>docker ps -aq<code></code></li></ul></li></ul></li> <li>How to preserve state ? <ul><li>let us have a dir <code>test</code> and it contains files in that directory</li> <li>what if we need to have access to the files from our docker container ?</li></ul></li> <li>Executing a file from inside volumes: <ul><li>We have a folder <code>test/</code> that should be available for both host machine and the container.</li> <li>Using volume mapping as : <code>docker run -it --entrypoint=bash -v $(pwd)/test:/app/test python:3.13.11-slim</code></li> <li>where <code>$(pwd)/test/</code> is the local fs directory where it is kept</li> <li>and <code>/app/test</code> is where it is being mapped to in Docker.</li> <li>remember this should be executed from <code>$(pwd)</code> and not inside from <code>$(pwd)/test</code></li></ul></li></ul> <h3>Data Pipelines:</h3> <ul><li>A data pipeline is a service that receives data as input and outputs more data.</li> <li>Eg: reading a CSV file, transforming the data somehow and storing it as a table in a PostgreSQL database.</li> <li><img src="/images/migrated/data-engineering-zoomcamp/imgs/Screenshot%202026-01-18%20at%201.17.04%20PM.png" alt="Screenshot 2026-01-18 at 1.17.04 PM.png"/></li> <li>We have CSV files and parses the CSV files and creates the parquert files -> takes in some input and produces some output.</li> <li>Anything that can take data from one and put in some destination is called a data pipeline.</li></ul> <h4>Building a basic pipeline in <code>pipeline/pipeline.py</code>:</h4> <ul><li>arguments to check data in month 12 using the sys module</li> <li>using pandas to then visualize the data in the given month</li> <li>then saving the data to parquet -> parquet is a binary format for data. optimized for csv</li> <li>want a isolated environment for setting up pyarrow and parquet.</li> <li>create an virtual environment for the environment , different from the virtual environment we have for the docker image.</li> <li>easier to isolate one project from another</li> <li><img src="/images/migrated/data-engineering-zoomcamp/imgs/Screenshot%202026-01-18%20at%201.35.58%20PM.png" alt="Screenshot 2026-01-18 at 1.35.58 PM.png"/></li> <li>using uv to manage creating virtual environments <ul><li><code>uv init --python 3.13</code></li> <li>it creates its own python and it will use the uv’s python to manage the project and to add the dependencies.</li></ul></li> <li>now we create an new docker image , python 3.13 , pyarrow, pandas and when we run docker , we run this pipeline.py</li> <li><code>Dockerfile</code> is a special file that tells how we are gonna create the Docker container.</li> <li>Run as \`docker build -t test:pandas . <ul><li><code>-t</code> is the tag name that we need to pass</li> <li><code>.</code> is the directory we are targetting ; where Dockerfile is specifically</li> <li>Then can run as <code>docker run -it --entrypoint=bash --rm test:pandas</code> <ul><li>when we finish the docker sessions, we wont have the state saved somewhere</li> <li>no have any dangling things left</li></ul></li> <li>Using <code>ENTRYPOINT</code> we can redefine where we want it to run it from next time. <ul><li><code>docker run -it --rm test:pandas 12</code></li></ul></li> <li>in our local machine we used <code>uv</code> but in our docker image we didnt. <ul><li>update our docker image  to copy from the host system to the docker image</li> <li><code>COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/</code></li> <li>after doing this uv becomes available.</li> <li>after copying this we run <code>RUN uv sync --locked</code> to have the same dependencies on the docker container</li> <li>so the python env in our local and in docker will exactly will be the same.</li> <li>we can pass the uv run command in the entrypoint</li> <li>or better to use it in the env path</li> <li><code>ENV PATH="/code/.venv/bin:$PATH"</code></li></ul></li></ul></li></ul> <h4>Adding Postgres to our flow:</h4> <ul><li>Docker can help with specific version of Postgres for our setup.</li> <li>Setting up basic postgres on docker:</li> <li><code>docker run -it --rm -e POSTGRES_USER="root" -e POSTGRES_PASSWORD="root" -e POSTGRES_DB="my_taxi" -v ny_taxi_postgres_data:/var/lib/postgresql -p 5432:5432 postgres:18</code></li> <li><code>-e</code> flag are used to setup env variables</li> <li>for postgres we are configuring it with a user , password and database name.</li> <li>this is different from every container , the potential variables can be used.</li> <li><code>ny_taxi_postgres_data</code> will be a inner volume for Docker that we will not be able to see <ul><li>with it we can preserve our data</li> <li>we can put whatever data that we want , and save it and next time it would be there</li> <li>The concept is the same with <strong>volume mapping</strong> <ul><li>we have a host machine and we map it to a container</li></ul></li> <li>With <strong>port mapping</strong> <ul><li>we have a port in host machine and we map it to a port in the container.</li> <li>we sent a request to <code>localhost:5432</code> , is actually sent to whatever is running to inside in the container.</li></ul></li> <li>pgcli <ul><li>python tool to intertact with postgres</li> <li>adding it as a dev dependency to the uv project.</li> <li>not a main dependency, using this dependency to just interaction with dev,not dependent on production use case.</li> <li><code>uv run pgcli -h localhost -p 5432 -u root -d ny_taxi</code></li></ul></li></ul></li> <li>Flow of things till now : <ul><li><img src="/images/migrated/data-engineering-zoomcamp/imgs/Screenshot%202026-01-18%20at%207.01.38%20PM.png" alt="Screenshot 2026-01-18 at 7.01.38 PM.png"/></li></ul></li> <li>Even if we disconnect from it now, it will still have all the data till now.</li></ul> <h4>Interactive Analysis of Our Data using Jupyter</h4> <ul><li>adding <code>jupyter</code> as a dev dependency to see and analyze the data into a jupyter notebook and to see the dependency.</li> <li>we want to use pandas to read a csv file — which is present in remote — nyt taxi dataset , which is in parquet.</li> <li>useful engineering tip -> added the prefix to set the main route to download from and url to point to specific dataset we want <ul><li>then use <code>pd.read_csv()</code> to download the specific url</li> <li>can then look at the data using <code>df.head()</code></li> <li>now our goal is to put this data into postgres.</li> <li>parquet also schemas in the datatype, but csv doesnt</li> <li>so pandas is trying to infer this type somehow</li> <li>so we use a custom datatype , so to remove the type not matching warning</li> <li><pre class="shiki vesper" style="background-color:#101010;color:#FFF"><code><span class="line"><span style="color:#FFF">	  dtype </span><span style="color:#A0A0A0">=</span><span style="color:#FFF"> {</span></span>
<span class="line"><span style="color:#99FFE4">    "VendorID"</span><span style="color:#FFF">: </span><span style="color:#99FFE4">"Int64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "passenger_count"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"Int64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "trip_distance"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"float64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "RatecodeID"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"Int64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "store_and_fwd_flag"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"string"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "PULocationID"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"Int64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "DOLocationID"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"Int64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "payment_type"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"Int64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "fare_amount"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"float64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "extra"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"float64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "mta_tax"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"float64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "tip_amount"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"float64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "tolls_amount"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"float64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "improvement_surcharge"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"float64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "total_amount"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"float64"</span><span style="color:#FFF">,</span></span>
<span class="line"><span style="color:#99FFE4">    "congestion_surcharge"</span><span style="color:#FFF">:</span><span style="color:#99FFE4">"float64"</span></span>
<span class="line"><span style="color:#FFF">}</span></span></code></pre></li> <li>to add this in our postgres , we would need an ORM :<code>sqlalchemy</code> and also the <code>psyciog2-binary</code> which is the postgres driver for python.</li> <li>first we create an engine in our jupyter notebook to ingest the data.</li> <li><pre class="shiki vesper" style="background-color:#101010;color:#FFF"><code><span class="line"><span style="color:#A0A0A0">from</span><span style="color:#FFF"> sqlalchemy </span><span style="color:#A0A0A0">import</span><span style="color:#FFF"> create_engine</span></span>
<span class="line"><span style="color:#FFF">engine </span><span style="color:#A0A0A0">=</span><span style="color:#FFF"> create_engine(</span><span style="color:#99FFE4">"postgresql://root:root@localhost:5432/ny_taxi"</span><span style="color:#FFF">)</span></span></code></pre> <ul><li>add the schema only (without inserting data)</li> <li><pre class="shiki vesper" style="background-color:#101010;color:#FFF"><code><span class="line"><span style="color:#FFF">df.head(</span><span style="color:#FFC799">0</span><span style="color:#FFF">).to_sql(name</span><span style="color:#A0A0A0">=</span><span style="color:#99FFE4">'yellow_taxi_data'</span><span style="color:#FFF">,con</span><span style="color:#A0A0A0">=</span><span style="color:#FFF">engine,if_exists</span><span style="color:#A0A0A0">=</span><span style="color:#99FFE4">'replace'</span><span style="color:#FFF">)</span></span></code></pre></li></ul></li></ul></li> <li>now to save this data into postgres , we should save them in chunks , otherwise the datasets could be huge and we dont want to load them up in memory.</li> <li>we define an iterator : <ul><li><pre class="shiki vesper" style="background-color:#101010;color:#FFF"><code><span class="line"><span style="color:#FFF">df_iter </span><span style="color:#A0A0A0">=</span><span style="color:#FFF"> pd.read_csv(url,dtype</span><span style="color:#A0A0A0">=</span><span style="color:#FFF">dtype,parse_dates</span><span style="color:#A0A0A0">=</span><span style="color:#FFF">parse_dates,</span></span>
<span class="line"><span style="color:#FFF">iterator</span><span style="color:#A0A0A0">=</span><span style="color:#FFF">True,chunksize</span><span style="color:#A0A0A0">=</span><span style="color:#FFC799">100000</span><span style="color:#FFF">)</span></span></code></pre></li> <li>and then using a for loop we can iterate and push the data into the database.</li></ul></li> <li>can then insert and check progress using <code>tqdm</code> <ul><li><pre class="shiki vesper" style="background-color:#101010;color:#FFF"><code><span class="line"><span style="color:#A0A0A0">for</span><span style="color:#FFF"> df_chunk </span><span style="color:#A0A0A0">in</span><span style="color:#FFF"> tqdm(df_iter):</span></span>
<span class="line"><span style="color:#FFF">df_chunk.to_sql(name</span><span style="color:#A0A0A0">=</span><span style="color:#99FFE4">'yellow_taxi_data'</span><span style="color:#FFF">,con</span><span style="color:#A0A0A0">=</span><span style="color:#FFF">engine,if_exists</span><span style="color:#A0A0A0">=</span><span style="color:#99FFE4">'append'</span><span style="color:#FFF">)</span></span></code></pre></li></ul></li></ul> <h4>Optimized Data Ingestion</h4> <ul><li>generate a script from the jupyter notebook and create an ingestion pipeline</li> <li>refactored into clean ingestion pipeline that can be run with a single command. <ul><li><code>python pipeline/ingest_data.py</code></li></ul></li></ul> <h4>Making a Better command line interface using click</h4> <ul><li>Using <code>click</code> to make a better command line interface to use our parameters to get the data and make it better.</li></ul> <h4>Architecture till now:</h4> <ul><li><img src="/images/migrated/data-engineering-zoomcamp/imgs/Screenshot%202026-01-18%20at%209.46.25%20PM.png" alt="Screenshot 2026-01-18 at 9.46.25 PM.png"/></li> <li>when we try to connect to the localhost , it is trying to connect to the localhost internally to docker not to our local.</li> <li>need to create using <code>docker network</code></li> <li>things within the same network can see each other. <ul><li><code>pgdatabase</code> in <code>postgres:18</code> container</li> <li><code>taxi_ingest:v001</code> in <code>ingest</code> container</li> <li><img src="/images/migrated/data-engineering-zoomcamp/imgs/Screenshot%202026-01-18%20at%209.53.50%20PM.png" alt="Screenshot 2026-01-18 at 9.53.50 PM.png"/></li> <li>they can then see each other and can also access ports of each other.</li> <li>after resolving this, we added pgAdmin to run in the same network to get a better UI.</li> <li>After adding pgadmin: <ul><li><img src="/images/migrated/data-engineering-zoomcamp/imgs/Screenshot%202026-01-18%20at%2010.04.45%20PM.png" alt="Screenshot 2026-01-18 at 10.04.45 PM.png"/></li></ul></li></ul></li></ul> <h4>docker-compose</h4> <ul><li>to execute both the services db up and setup pgAdmin together.</li> <li>creates volumes and networks</li></ul> <h3>SQL Refresher</h3> <ul><li></li></ul>`);
      },
      $$slots: { default: true }
    }
  ]));
}
const __vite_glob_0_18 = /* @__PURE__ */ Object.freeze(/* @__PURE__ */ Object.defineProperty({
  __proto__: null,
  default: Lec1_md,
  metadata
}, Symbol.toStringTag, { value: "Module" }));
export {
  __vite_glob_0_18 as _
};
