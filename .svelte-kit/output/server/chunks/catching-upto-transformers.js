import { G as sanitize_props, J as spread_props } from "./index.js";
import { M as MarkdownLayout } from "./MarkdownLayout.js";
const metadata = {
  "title": "Fast Tracking the Course of AI",
  "tags": ["ai", "nlp", "transformers", "100xdevs"],
  "date": "1/23/26"
};
const { title, tags, date } = metadata;
function Catching_upto_transformers_md($$renderer, $$props) {
  const $$sanitized_props = sanitize_props($$props);
  MarkdownLayout($$renderer, spread_props([
    $$sanitized_props,
    metadata,
    {
      children: ($$renderer2) => {
        $$renderer2.push(`<h3>Introduction</h3> <ul><li>field is moving really really fast</li> <li>understand how modern AI engineering is done</li> <li>tools , use cases , agents ,skills - subset of buzz words - a lot of other things are present.</li> <li>ask lots of intuitive questions during interviews and also learn about the interview process as well</li> <li>understand how LLMs actually work</li> <li>research also involves implementing a lot of stuff that you work with daily</li> <li>things to do <ul><li>basic python</li> <li>have a colab account ready</li> <li>look at different models available in the market today and explore them</li> <li>lot of models in the market -> try to also look at os models like qwen, deepseek etc. <ul><li>how different they are from other models.</li></ul></li></ul></li> <li>free stuff we can try: <ul><li>cli tools people are getting from that</li> <li>opencode</li> <li>gemini cli</li></ul></li> <li>building and being good with the fundamentals is more important than doing a phd now.</li> <li>applied ai <ul><li>using existing tools, how to make things that are better to use and to find new applications that can be built on top of them.</li></ul></li> <li>what are some good projects ? <ul><li>dealing with los of data</li> <li>lots of context, lots of data</li> <li>how we pass the data to the llm</li> <li>memory and need of something -> mix and match of these</li></ul></li> <li>context engineering <ul><li>when to use which model</li> <li>what are the best models</li> <li>how to switch contexts etc.</li></ul></li> <li>agentic tasks, data pipeling and building webapps on top of a agentic skill and definitely a good way to learn about this</li> <li>go on colab and learn and type everything <ul><li>theory and learn about things</li> <li>and what direction we are going towards.</li></ul></li> <li>coding simple transformations and simple blocks of attention</li> <li>go out and try qwen, deepseek , llama on openrouter and find examples on where where these models differ , different guard rails and jailbreaking these open access models.</li> <li>what do AI engineers do ? <ul><li>new “backend” engineer</li> <li>pace we are moving is the same thing</li> <li>every company needs someone who can do this and has become a new completely different field and high time to learn what this tools are</li> <li>lot of people are vibe coding things and moving fast.</li> <li>can move in 2-3 projects at two times and deliver more than asked for and using these agents much easier to build and deploy faster.</li></ul></li> <li>openrouter and opencode to play around with different models.</li></ul> <h3>Fast Tracking the Course of AI:</h3> <ul><li>intelligence <ul><li>ability to achieve goals in a wide range of situations.</li></ul></li> <li><code>&lt;History /></code> of AI and other stuff</li> <li>why language is hard for computers ? <ul><li>Natural Language Processing</li> <li>4 different attempts to NLP <ul><li>Put up every possible word in the dictionary approach in it -> n-gram model</li> <li>Statistical Pattern based techniques -> Pattern Matching != Understanding <ul><li>Machine could predict the next word based on frequency, but it had no concept of what the words actually meant.</li> <li>Breakthrough -> Words as Numbers (computers can also understand numbers), words doesnt make any implicit sense for computers.</li> <li>Challenge -> how do we turn a word into a list of numbers that captures its meaning ?</li> <li><code>Apple</code> -> <code>[0.92,-0.14,0.05,....]</code></li> <li>Hint: these are called embeddings and we are creating embeddings.</li></ul></li> <li>Word Embeddings -> “Secret Sauce” <ul><li>How do we turn a word like “Apple” into a number that captures its meaning ?</li> <li>Capture the meaning not in 1d space but in a higher dimensional space</li> <li>Instead of one number, we give each word a list of numbers(a vector).</li> <li>Each number represents a specific “dimension” of meaning.</li> <li>very big breakthrough as for every word, it can be very high.</li></ul></li> <li>Eg: king and queen have similar numbers for royalty, but vastly different values for gender. This is how the machines represent concepts as data.</li> <li>![[Screenshot 2026-01-23 at 10.55.13 AM.png]]</li> <li>Words as Positions in Space <ul><li>If a word is a list of numbers, its a point on a map. Similar words are close together, while different words are far apart.</li></ul></li> <li>![[Screenshot 2026-01-23 at 10.57.45 AM.png]]</li> <li>So , similar words are placed close together in a multi-dimensional space. Proximity indicates shared meaning.</li></ul></li></ul></li></ul> <h4>magic of embeddings</h4> <ul><li>word math <ul><li>king - man + woman  = queen</li> <li>paris - france + italy = rome</li> <li>walking  - walk + swim = swimming</li></ul></li> <li>can embeddings solve our earlier problems ? <ul><li>in basic word embeddings, each word has exactly ONE position in space.</li> <li>“I went to the bank to deposit cash”</li> <li>“I sat on the bank of the river”</li></ul></li></ul> <h4>Sequence Models (RNNs)</h4> <ul><li>okay we can make embeddings now, but what to make of that embedding exactly.</li> <li>we are not able to make use of the embedding and use it anyhow.</li> <li>Idea: Process the sentence one word at a time, building up understanding as we go. <ul><li>Model maintains a “memory” of what it has read so far.</li></ul></li> <li>Eg: other models like GRUS, LSTMS etc. and other divergent fields like Mambas</li> <li>Even in fact , Transformers are the best version of top of the sequence models.</li> <li>Its a architecture , that deals with the sequence and from each word they retain some memory.</li> <li>the context and upcoming word , change the meaning of our word completely.</li> <li>Problem : Forgetting <ul><li>World ,storage and memroy is finite.</li> <li>And it has to go back a long vectors back.</li> <li>meaning would completey disrupt as it goes back a lot of tokens.</li></ul></li> <li>RNNs struggle with long sentences because they process information linearly and have limited memory capacity. <ul><li>LSTM came up after RNN, added gates for choice to add to its memory and the memory and the information that it needs to retain is very less.</li> <li>GRUS came after that</li></ul></li></ul> <h3>Transformers</h3> <ul><li>stage is set around 2017</li> <li>what we have till now <ul><li>word embeddings</li> <li>sequence models(RNNs)</li> <li>Tons of internet data</li> <li>Powerful GPUs</li></ul></li> <li>We are just lacking in a powerful way to make all these work better.</li> <li>(old idea) process words one by one, like reading a book from start to end.</li> <li>The idea of transformer <ul><li>Simultaneous <ul><li>Look at every word in the sentence at the same time.</li></ul></li> <li>Attention <ul><li>model “attends” to the most relevant words, no matter how far apart they are.</li></ul></li></ul></li> <li>Have to make them more selective and it was wasting more of their information.</li> <li>And the second thing that we need to do is , all the word that we are processing should be done parallely. <ul><li>Attention enables the model to focus only on the most relevant word.</li></ul></li></ul> <h4>Attention in Action</h4> <ul><li>when the model looks at the focus word, the attention mechanism tells it to pay the most attention to it.</li> <li>![[Screenshot 2026-01-23 at 12.58.46 PM.png]]</li> <li>how it happens depends on the maths behind it</li> <li>Model “knows” what the pronouns refers to buiding a context aware representation of every word is.</li> <li>Why Attention is powerful : <ul><li>More memory -> every word in a sentence “sees” every other word simultaneously , no matter how far apart they are</li> <li>Parallel Processing -> Unlike RNNs that read word by word, Transformers process all words at once, making training incredibly fast.</li> <li>Deep Understanding -> The model builds a mathematical map of how every word relates to every other word in the specific context.</li></ul></li> <li>![[Screenshot 2026-01-23 at 7.38.54 PM.png]]</li> <li>By changing just one word (tired vs wide), the model’s attention shifts, correctly identifying what “it” refers to in each context.</li> <li>Whole Flow (of a transformer based language model): <ul><li>![[Screenshot 2026-01-23 at 7.49.03 PM.png]]</li> <li>takes the sentence , outputs a list of words and adds to the sentence and basically repeats itself until the MAX_TOKEN_LENGTH that its allowed.</li></ul></li> <li>Why “Predict the Next Word” Creates Understanding <ul><li>Grammar and Syntax <ul><li>Predicting the next word requires the model to internalize language structure and rules.</li></ul></li> <li>Factual Knowledge <ul><li>Correct predictions rely on learned factual relations between concepts.</li></ul></li> <li>Logic and Understanding <ul><li>Following a chain of reasoning lets the model anticipate the appropriate continuation.</li></ul></li></ul></li> <li>Generation of Text <ul><li>![[Screenshot 2026-01-23 at 8.04.31 PM.png]]</li> <li>Generation is a iterative loop.</li> <li>Each predicted word is added back to the input and the model predicts the next word based on the updated context.</li></ul></li></ul> <h3>Foundation Models</h3> <ul><li>“Foundation” models are pre-trained models based on huge corpus of data and training loop.</li> <li>It’s kind of like the base knowledge.</li> <li>We no longer build tools from scratch, we build on top of these giants.</li> <li>General Purpose AI</li></ul> <h3>Current Directions (2024-25)</h3> <ul><li>Multimodality <ul><li>AI is no longer just text. It can see images, hear voices, and speak back in real time.</li></ul></li> <li>Reasoning <ul><li>How models are designed to “think” before they speak, solving complex math and logic problems.</li></ul></li> <li>Agents <ul><li>shift from chatbot to agents that can use tools, browse the we, and complete multi-step tasks</li> <li>different applications that we see on the internet that use some source of LLM <ul><li>take input from user, use some LLM and building on top of that.</li></ul></li></ul></li></ul>`);
      },
      $$slots: { default: true }
    }
  ]));
}
const __vite_glob_0_12 = /* @__PURE__ */ Object.freeze(/* @__PURE__ */ Object.defineProperty({
  __proto__: null,
  default: Catching_upto_transformers_md,
  metadata
}, Symbol.toStringTag, { value: "Module" }));
export {
  __vite_glob_0_12 as _
};
